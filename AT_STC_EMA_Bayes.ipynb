{"cells": [{"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "zBmBOQsuwH4e", "outputId": "ef839268-884d-44a3-b1dc-db492ea84dc5"}, "cell_type": "code", "source": "!pip install --proxy http://proxyaws-qvr.pole-emploi.intra:8080 yfinance ta bayesian-optimization", "execution_count": 1, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: yfinance in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (0.2.18)\nRequirement already satisfied: ta in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (0.10.2)\nRequirement already satisfied: bayesian-optimization in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (1.4.3)\nRequirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (4.12.2)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (1.20.3)\nRequirement already satisfied: pandas>=1.3.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (1.3.4)\nRequirement already satisfied: frozendict>=2.3.4 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (2.3.8)\nRequirement already satisfied: html5lib>=1.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (1.1)\nRequirement already satisfied: appdirs>=1.4.4 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (1.4.4)\nRequirement already satisfied: pytz>=2022.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (2023.3)\nRequirement already satisfied: multitasking>=0.0.7 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (0.0.11)\nRequirement already satisfied: cryptography>=3.3.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (3.4.8)\nRequirement already satisfied: lxml>=4.9.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (4.9.2)\nRequirement already satisfied: requests>=2.26 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from yfinance) (2.26.0)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from bayesian-optimization) (1.7.3)\nRequirement already satisfied: colorama>=0.4.6 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from bayesian-optimization) (0.4.6)\nRequirement already satisfied: scikit-learn>=0.18.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from bayesian-optimization) (1.0.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.1)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from cryptography>=3.3.2->yfinance) (1.14.6)\nRequirement already satisfied: pycparser in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\nRequirement already satisfied: webencodings in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\nRequirement already satisfied: six>=1.9 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (1.15.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests>=2.26->yfinance) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests>=2.26->yfinance) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests>=2.26->yfinance) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests>=2.26->yfinance) (3.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.17.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n", "output_type": "stream"}]}, {"metadata": {"id": "QKr9IKYRwpbC"}, "cell_type": "code", "source": "import pandas as pd, numpy as np\nimport itertools\nimport seaborn as sns\nimport yfinance as yf\nimport multiprocessing as mp\nimport time\nimport logging\n\nfrom copy import copy\nimport statistics as stats\nimport math\nfrom functools import reduce\n\nfrom ta.volume import MFIIndicator\nfrom ta.volatility import AverageTrueRange\nfrom ta.trend import STCIndicator\nfrom ta.trend import EMAIndicator\n\nfrom bayes_opt import BayesianOptimization", "execution_count": 2, "outputs": []}, {"metadata": {"id": "J3HFB0dMU5gZ"}, "cell_type": "markdown", "source": "# G\u00e9n\u00e9ration du fichier source\n"}, {"metadata": {"id": "fbd7c74ed98b4f0a87a453b36eb7bbff"}, "cell_type": "code", "source": "from ibm_watson_studio_lib import access_project_or_space\nwslib = access_project_or_space()\n\ndf_historical_data = pd.read_csv(wslib.mount.get_data_path('NDX_1985.csv'))\ndf_historical_data.reset_index(inplace=True)\ndf_historical_data.drop(columns=[\"index\"], inplace=True)\ndf_historical_data.head()", "execution_count": 3, "outputs": [{"data": {"text/plain": "         Date        Open        High         Low       Close   Adj Close  \\\n0  1985-10-01  110.620003  112.160004  110.565002  112.139999  112.139999   \n1  1985-10-02  112.139999  112.540001  110.779999  110.824997  110.824997   \n2  1985-10-03  110.839996  111.184998  110.120003  110.870003  110.870003   \n3  1985-10-04  110.870003  110.870003  109.855003  110.074997  110.074997   \n4  1985-10-07  110.074997  110.135002  108.175003  108.199997  108.199997   \n\n      Volume  \n0  153160000  \n1  164640000  \n2  147300000  \n3  147900000  \n4  128640000  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1985-10-01</td>\n      <td>110.620003</td>\n      <td>112.160004</td>\n      <td>110.565002</td>\n      <td>112.139999</td>\n      <td>112.139999</td>\n      <td>153160000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1985-10-02</td>\n      <td>112.139999</td>\n      <td>112.540001</td>\n      <td>110.779999</td>\n      <td>110.824997</td>\n      <td>110.824997</td>\n      <td>164640000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1985-10-03</td>\n      <td>110.839996</td>\n      <td>111.184998</td>\n      <td>110.120003</td>\n      <td>110.870003</td>\n      <td>110.870003</td>\n      <td>147300000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1985-10-04</td>\n      <td>110.870003</td>\n      <td>110.870003</td>\n      <td>109.855003</td>\n      <td>110.074997</td>\n      <td>110.074997</td>\n      <td>147900000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1985-10-07</td>\n      <td>110.074997</td>\n      <td>110.135002</td>\n      <td>108.175003</td>\n      <td>108.199997</td>\n      <td>108.199997</td>\n      <td>128640000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}, "execution_count": 3, "output_type": "execute_result"}]}, {"metadata": {"id": "93dbd879e7d143789e56c8432434a7b3"}, "cell_type": "code", "source": "df_historical_data.dtypes", "execution_count": 4, "outputs": [{"data": {"text/plain": "Date          object\nOpen         float64\nHigh         float64\nLow          float64\nClose        float64\nAdj Close    float64\nVolume         int64\ndtype: object"}, "metadata": {}, "execution_count": 4, "output_type": "execute_result"}]}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ecBhrvulVWmm", "outputId": "19139448-74f3-4f0c-800a-73a5fe50fe5d"}, "cell_type": "code", "source": "# Test si aucune ligne manquante\ntest_list = [champ == 0 for champ in df_historical_data.isnull().sum()]\n\n# Si toutes les colonnes sont True, r\u00e9sultat = True\nnotnull = all(i for i in test_list)\nprint(\"Aucune ligne vide d\u00e9tect\u00e9e, pour l'ensemble des colonnes : \", notnull)", "execution_count": 5, "outputs": [{"name": "stdout", "text": "Aucune ligne vide d\u00e9tect\u00e9e, pour l'ensemble des colonnes :  True\n", "output_type": "stream"}]}, {"metadata": {"id": "_8f2OCnYykoW"}, "cell_type": "code", "source": "df_historical_data[\"Open\"] = df_historical_data.Open.apply(lambda x: round(x,2))\ndf_historical_data[\"High\"] = df_historical_data.High.apply(lambda x: round(x,2))\ndf_historical_data[\"Low\"] = df_historical_data.Low.apply(lambda x: round(x,2))\ndf_historical_data[\"Close\"] = df_historical_data.Close.apply(lambda x: round(x,2))", "execution_count": 6, "outputs": []}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "XF7mNsSXVfbK", "outputId": "f415bb3d-221f-46b4-ab81-979db6a4304d"}, "cell_type": "code", "source": "df_historical_data.tail(1)", "execution_count": 7, "outputs": [{"data": {"text/plain": "            Date      Open      High       Low     Close     Adj Close  \\\n9474  2023-05-04  13014.07  13064.02  12938.45  12982.48  12982.480469   \n\n          Volume  \n9474  4745780000  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9474</th>\n      <td>2023-05-04</td>\n      <td>13014.07</td>\n      <td>13064.02</td>\n      <td>12938.45</td>\n      <td>12982.48</td>\n      <td>12982.480469</td>\n      <td>4745780000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}, "execution_count": 7, "output_type": "execute_result"}]}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "FjsWbKF3Vr2c", "outputId": "5c3c64fc-5270-4c8f-9b14-653c004a0b6d"}, "cell_type": "code", "source": "df_historical_data.dtypes", "execution_count": 8, "outputs": [{"data": {"text/plain": "Date          object\nOpen         float64\nHigh         float64\nLow          float64\nClose        float64\nAdj Close    float64\nVolume         int64\ndtype: object"}, "metadata": {}, "execution_count": 8, "output_type": "execute_result"}]}, {"metadata": {"id": "pTBQpw9u2Fzo"}, "cell_type": "markdown", "source": "# Fonctions"}, {"metadata": {"id": "XxWuUYApKXyq"}, "cell_type": "markdown", "source": "## Indicateurs Techniques"}, {"metadata": {"id": "bJUTIaxyVz6B"}, "cell_type": "markdown", "source": "#### Alphatrend"}, {"metadata": {"id": "W53ner8PX6Ay"}, "cell_type": "code", "source": "# Trend indicator, \u00e9quivalent de l'affichage couleur\ndef trend_indicator(trend):\n    if trend > 0 :\n        # Uptrend\n        x = 1\n    elif trend < 0 :\n        # Downtrend\n        x = -1\n    else :\n        # Range\n        x = 0\n    return x", "execution_count": 9, "outputs": []}, {"metadata": {"id": "Wm2pANWHVvw6"}, "cell_type": "code", "source": "# Defintion fonction\ndef generate_alphatrend(df_in, mfi_p, mfi_seuil, atr_l, m):\n    '''Param\u00e8tres d'entr\u00e9e : longueur MFI, longueur ATR, multiplier\n    Retourne les colonnes Alphatrend, Alphatrend +2, Trend (position AT1 / AT2)\n    :mfi_p = p\u00e9riode MFI servant \u00e0 d\u00e9limiter up/down de l'alphatrend\n    :mfi_seuil = p\u00e9riode MFI pour recherche crossover, d\u00e9termine uptrend ou downtrend'''\n\n    df = df_in.copy()\n\n    # Colonnes MFI\n    s_mfi = MFIIndicator(high=df.High, low=df.Low, close=df.Close, volume=df.Volume, window=mfi_p).money_flow_index()\n    df[\"MFI_ref\"] = s_mfi\n\n    # Colonne ATR\n    s_atr = AverageTrueRange(high=df.High, low=df.Low, close=df.Close, window=atr_l).average_true_range()\n    df[\"ATR\"] = s_atr\n\n    # Lignes UpT et DownT\n    df[\"UpT_support\"] = df[\"Low\"] - df[\"ATR\"] * m\n    df[\"DownT_support\"] = df[\"High\"] + df[\"ATR\"] * m\n\n    # Suppression des lignes sans signal, en d\u00e9but de DataFrame\n    df.dropna(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\n    # ===============================================\n    # Calcul Alphatrend, en tant que s\u00e9rie\n    \n    Alphatrend = [0]\n\n    for i in range (1, df.shape[0]):\n        # Cas Uptrend\n        if df.at[i,\"MFI_ref\"] >= mfi_seuil :\n            if df.at[i,\"UpT_support\"] < Alphatrend[-1] :\n                # Flat\n                Alphatrend.append(Alphatrend[-1])\n            else :\n                # Trailing stop loss Up\n                Alphatrend.append(df.at[i,\"UpT_support\"])\n\n        # Cas Downtrend, MFI < 50\n        else :\n            if df.at[i,\"DownT_support\"] > Alphatrend[-1] :\n                # Flat\n                Alphatrend.append(Alphatrend[-1])\n            else :\n                # Trailing stop loss Down\n                Alphatrend.append(df.at[i,\"DownT_support\"])\n\n    # ===============================================\n    # Ajout des lignes k1 et k2 en tant que colonnes\n    \n    if df.shape[0] == len(Alphatrend):\n        df[\"Alphatrend_k1\"] = pd.Series(Alphatrend).apply(lambda x: round(x,2))\n        # Ligne k2 d\u00e9cal\u00e9e de 2j\n        Alphatrend2 = df[\"Alphatrend_k1\"].shift(periods=2, fill_value=0)\n        df[\"Alphatrend_k2\"] = pd.Series(Alphatrend2).apply(lambda x: round(x,2))\n        # Trend\n        df[\"Trend\"] = df.Alphatrend_k1 - df.Alphatrend_k2\n        df[\"Trend\"] = df[\"Trend\"].apply(trend_indicator)\n    else :\n        print(\"Erreur lors de la g\u00e9n\u00e9ration des lignes Alphatrend\")\n\n    # ===============================================\n    # G\u00e9n\u00e9ration des signaux Achat / Vente\n\n    # On isole tous les index non neutres, o\u00f9 AT1 != AT2, \u00e0 la hausse (1) comme \u00e0 la baisse (-1)\n    s_trend = df[\"Trend\"].loc[df[\"Trend\"]!=0]\n    s_trend_diff = s_trend - s_trend.shift(1)\n\n    buy_signal_indexes = s_trend_diff[s_trend_diff == 2].index\n    sell_signal_indexes = s_trend_diff[s_trend_diff == -2].index\n\n    df[\"Signal\"] = 0\n    df.loc[buy_signal_indexes,\"Signal\"] = 1\n    df.loc[sell_signal_indexes,\"Signal\"] = -1\n\n    # ===============================================\n    # S\u00e9lection des colonnes suffisantes\n    df = df[[\"Date\",\"Alphatrend_k1\",\"Alphatrend_k2\",\"Trend\",\"Signal\"]]\n    \n    return df", "execution_count": 10, "outputs": []}, {"metadata": {"id": "EbCqhjaFcBXo"}, "cell_type": "markdown", "source": "#### STC & EMA"}, {"metadata": {"id": "Kr56pqQgdycS"}, "cell_type": "code", "source": "def generate_STC_and_EMA(df_in, stc_length, fast_length, slow_length, ema_period):\n  \n    df = df_in[[\"Date\",\"Close\"]].copy()\n\n    s_stc = STCIndicator(close=df.Close, window_slow=slow_length, window_fast=fast_length, cycle=stc_length).stc()\n    s_ema = EMAIndicator(close=df.Close, window=ema_period).ema_indicator()\n\n    df[\"STC\"] = round(s_stc,2)\n    df[\"EMA\"] = round(s_ema,2)\n\n    df.drop(columns=[\"Close\"], inplace=True)\n\n    return df", "execution_count": 11, "outputs": []}, {"metadata": {"id": "rYMgPLFf-WVU"}, "cell_type": "markdown", "source": "#### ATR sortie & Merge tous indicateurs techniques"}, {"metadata": {"id": "EGM8g_il2CNh"}, "cell_type": "code", "source": "def merge_technical_indicators(df_in, atr_l, df1, df2, date_min=\"1998-01-01\"):\n    ''' Fusionne les DataFrames d'indicateurs techniques, \n    ajoute \u00e9galement date_min au format 'yyyy-mm-dd' pour fixer le d\u00e9but du Backtesting'''\n\n    df_essentials = df_in.copy()\n\n    # Ajout de la colonne ATR qui servira plus tard dans le calcul de la sortie.\n    s_atr = AverageTrueRange(high=df_essentials.High, low=df_essentials.Low, close=df_essentials.Close, window=atr_l).average_true_range()\n    df_essentials[\"ATR\"] = pd.Series(s_atr).apply(lambda x: round(x,2))\n\n    # R\u00e9duction au strict n\u00e9cessaire pour les colonnes\n    df_essentials = df_essentials[[\"Date\",\"Open\",\"Close\",\"ATR\"]].copy()\n\n    # Merge des 3\n    data_frames = [df_essentials, df1, df2]\n    df_merged = reduce(lambda  left,right: pd.merge(left,right, on=['Date'], how='left'), data_frames)\n\n    # R\u00e9duction de la fen\u00eatre de tests \u00e0 partir de la date_min\n    df_merged = df_merged.loc[df_merged[\"Date\"] >= date_min]\n\n    df_merged.reset_index(drop=True,inplace=True)\n\n    return df_merged", "execution_count": 12, "outputs": []}, {"metadata": {"id": "u35LtIZtKcBh"}, "cell_type": "markdown", "source": "## Entr\u00e9es / Sorties "}, {"metadata": {"id": "9qXkxVTCJg2Q"}, "cell_type": "markdown", "source": "#### D\u00e9tection des entr\u00e9es"}, {"metadata": {"id": "RWCU9QZoQ1u-"}, "cell_type": "markdown", "source": "Strat\u00e9gie :<br>\n<li>Entre\u00e9e : Buy signal + Prix > EMA + STC < seuil(25)</li>\n<li>Sortie : Sell signal + Prix < EMA + STC > seuil(75)</li>"}, {"metadata": {"id": "E3TtPoWKJZry"}, "cell_type": "markdown", "source": "#### Sorties, valeurs & aggr\u00e9gation"}, {"metadata": {"id": "EF8YaZmfJr4V"}, "cell_type": "code", "source": "def generate_exit_vars(df_in, entry_index, atr_sl, rr_ratio):\n    ''' Obtient l'Open de l'index d'entr\u00e9e.\n    Puis g\u00e9n\u00e8re la valeur du Stop Loss et Take Profit'''\n\n    df = df_in.copy()\n\n    valeur_entree = df.at[entry_index,\"Open\"]\n    date_entree = df.at[entry_index,\"Date\"]\n    atr_reference = df.at[entry_index,\"ATR\"]\n\n    # Ajust\u00e9s pour Strat\u00e9gie Short\n    stop_loss = valeur_entree + atr_sl * atr_reference\n    take_profit = valeur_entree - rr_ratio * (atr_sl * atr_reference)\n\n    return date_entree, valeur_entree, stop_loss, take_profit", "execution_count": 13, "outputs": []}, {"metadata": {"id": "e2af5cb8b8d34538a88ea8e6786a036a"}, "cell_type": "code", "source": "def generate_exit_vars_Long(df_in, entry_index, atr_sl, rr_ratio):\n    ''' Obtient l'Open de l'index d'entr\u00e9e.\n    Puis g\u00e9n\u00e8re la valeur du Stop Loss et Take Profit'''\n\n    df = df_in.copy()\n\n    valeur_entree = df.at[entry_index,\"Open\"]\n    date_entree = df.at[entry_index,\"Date\"]\n    atr_reference = df.at[entry_index,\"ATR\"]\n\n    # Ajust\u00e9s pour Strat\u00e9gie Long\n    stop_loss = valeur_entree - atr_sl * atr_reference\n    take_profit = valeur_entree + rr_ratio * (atr_sl * atr_reference)\n\n    return date_entree, valeur_entree, stop_loss, take_profit", "execution_count": 14, "outputs": []}, {"metadata": {"id": "heaKiQ7LKzf5"}, "cell_type": "code", "source": "def return_SL_or_TP_index(df_in, entry_index, stop_loss, take_profit) :\n    '''Serie des valeurs close entre entr\u00e9e et fin du DataFrame.\n    Si Close > Stop loss, dans le cas d'un Short, alors touch\u00e9.\n    On cherche alors l'index de la premi\u00e8re valeur >=0, si il existe'''\n\n    df = df_in.copy()\n\n    last_line = max(df.index)\n\n    # Recherche index Stop Loss, ou index derni\u00e8re ligne df\n    # SHORT : Stop Loss touch\u00e9 si Close >= valeur SL\n    close_series = (df.loc[entry_index:last_line,\"Close\"] - stop_loss) >= 0\n    if len(close_series[ close_series == True ]) > 0 :\n        sl_index = close_series[ close_series == True ].index[0]\n    else :\n        # Pour la derni\u00e8re entr\u00e9e, si rien n'est touch\u00e9, alors la derni\u00e8re ligne fera office de sortie\n        sl_index = last_line\n\n    # Recherche indexTake Profit\n    # SHORT : Take Profit atteint si Close <= valeur TP\n    close_series = (take_profit - df.loc[entry_index:last_line,\"Close\"]) >= 0\n    if len(close_series[ close_series == True ]) > 0 :\n        tp_index = close_series[ close_series == True ].index[0]\n    else :\n        tp_index = last_line\n\n    # S\u00e9lection du 1er index touch\u00e9 : Stop Loss ou Take Profit\n    lowest_index = min(sl_index, tp_index)\n\n    # Pointeur des valeurs Date sortie & Prix de cl\u00f4ture dans le DataFrame indicateurs techniques\n    exit_date = df.at[lowest_index,\"Date\"]\n    exit_value = df.at[lowest_index,\"Close\"]\n\n    return exit_date, exit_value", "execution_count": 15, "outputs": []}, {"metadata": {"id": "d6728606a81044ed8726254407091035"}, "cell_type": "code", "source": "def return_SL_or_TP_index_Long(df_in, entry_index, stop_loss, take_profit) :\n    '''Serie des valeurs close entre entr\u00e9e et fin du DataFrame.\n    Si Close > Stop loss, dans le cas d'un Short, alors touch\u00e9.\n    On cherche alors l'index de la premi\u00e8re valeur >=0, si il existe'''\n\n    df = df_in.copy()\n\n    last_line = max(df.index)\n\n    # Recherche index Stop Loss, ou index derni\u00e8re ligne df\n    # LONG : Stop Loss touch\u00e9 si Close <= valeur SL\n    close_series = (df.loc[entry_index:last_line,\"Close\"] - stop_loss) <= 0\n    if len(close_series[ close_series == True ]) > 0 :\n        sl_index = close_series[ close_series == True ].index[0]\n    else :\n        # Pour la derni\u00e8re entr\u00e9e, si rien n'est touch\u00e9, alors la derni\u00e8re ligne fera office de sortie\n        sl_index = last_line\n\n    # Recherche indexTake Profit\n    # LONG : Take Profit atteint si Close >= valeur TP\n    close_series = (take_profit - df.loc[entry_index:last_line,\"Close\"]) <= 0\n    if len(close_series[ close_series == True ]) > 0 :\n        tp_index = close_series[ close_series == True ].index[0]\n    else :\n        tp_index = last_line\n\n    # S\u00e9lection du 1er index touch\u00e9 : Stop Loss ou Take Profit\n    lowest_index = min(sl_index, tp_index)\n\n    # Pointeur des valeurs Date sortie & Prix de cl\u00f4ture dans le DataFrame indicateurs techniques\n    exit_date = df.at[lowest_index,\"Date\"]\n    exit_value = df.at[lowest_index,\"Close\"]\n\n    return exit_date, exit_value", "execution_count": 16, "outputs": []}, {"metadata": {"id": "IPtvYjEzzM9s"}, "cell_type": "markdown", "source": "# Strategy as Class"}, {"metadata": {"id": "VHR08NixzMaH"}, "cell_type": "code", "source": "class Strat_AT_STC_EMA:\n  \n    def __init__(self, p_ema=200, p_AT_m=1, p_AT_l=14, p_AT_mfi_l = 14, p_AT_mfi_s = 50, p_STC_l=80, p_STC_slow_l=50, p_STC_fast_l=27, p_STC_b=25, p_STC_h=75, p_ATR_SL_l = 14, p_ATR_SL = 2, p_RR_ratio = 3, p_leverage=1):\n        self.ema_l = p_ema\n        self.at_m = p_AT_m\n        self.at_l = p_AT_l\n        self.at_mfi_l = p_AT_mfi_l\n        self.at_mfi_s = p_AT_mfi_s\n        self.stc_l = p_STC_l\n        self.stc_s_l = p_STC_slow_l\n        self.stc_f_l = p_STC_fast_l\n        self.stc_seuil_b = p_STC_b\n        self.stc_seuil_h = p_STC_h\n        self.ATR_SL_l = p_ATR_SL_l\n        self.ATR_SL = p_ATR_SL\n        self.RR_ratio = p_RR_ratio\n        self.leverage = p_leverage\n\n  \n    def make_technical_indicators(self, df_source):\n        df_AT = generate_alphatrend(df_source, mfi_p=self.at_mfi_l, mfi_seuil=self.at_mfi_s, atr_l=self.at_l, m=self.at_m)\n        df_STC_EMA = generate_STC_and_EMA(df_source, stc_length=self.stc_l, fast_length=self.stc_f_l, slow_length=self.stc_s_l, ema_period=self.ema_l)\n        df_Technical_Indicators = merge_technical_indicators(df_source, self.ATR_SL_l, df_AT, df_STC_EMA)\n        return df_Technical_Indicators\n\n\n    def get_entries_signals(self, df_in):\n        ''' N\u00e9cessite en entr\u00e9e le DataFrame avec indicateurs techniques.\n        L'enrichit avec signaux Entr\u00e9e Long (1) et Entr\u00e9e Short (-1).'''\n\n        df_IT = df_in.copy()\n\n        # Valeur 3 pour signaux d'entr\u00e9e valides\n        df_IT[\"Buy_entry\"] = np.sign(df_IT.Close - df_IT.EMA) + df_IT.Signal + np.sign(self.stc_seuil_b - df_IT.STC)\n        # Valeur -3 pour signaux d'entr\u00e9e valides\n        # Attention / par deux signaux n\u00e9gatifs -> positif, d'o\u00f9 l'inversion sur un seul champ\n        df_IT[\"Sell_entry\"] = np.sign(df_IT.Close - df_IT.EMA) + df_IT.Signal + np.sign(self.stc_seuil_h - df_IT.STC)\n\n        # Conversion en np array\n        arr_buy_entry = df_IT[\"Buy_entry\"].to_numpy()\n        # np.where(condition, vrai, sinon)\n        df_IT[\"Buy_entry\"] = np.where(arr_buy_entry==3, 1, 0)\n\n        arr_sell_entry = df_IT[\"Sell_entry\"].to_numpy()\n        df_IT[\"Sell_entry\"] = np.where(arr_sell_entry==-3.0, -1, 0)\n\n        # Agr\u00e9gation des deux types de signaux.\n        df_IT[\"Entry\"] = df_IT[\"Sell_entry\"] + df_IT[\"Buy_entry\"]\n        df_IT.drop(columns=[\"Buy_entry\",\"Sell_entry\"], inplace=True)\n\n        return df_IT\n\n  \n    def apply_short_strategy(self, df_entries):\n        ''' Entr\u00e9e : DataFrame avec indicateurs techniques g\u00e9n\u00e9r\u00e9 par get_entries_signals\n        Sortie : DataFrame avec entr\u00e9e - sortie, valeur & date'''\n\n        df = df_entries.copy()\n        d_entrees_sorties = {\n            \"date_entree\" : [],\n            \"valeur_entree\" : [],\n            \"date_sortie\" : [],\n            \"valeur_sortie\" : []\n        }\n\n        # Liste des entr\u00e9es : -1 pour signal Short\n        short_entries_indexes = df.loc[ df[\"Entry\"]==-1 ].index\n        # Ajout d'une unit\u00e9 pour entr\u00e9e le lendemain du signal confirm\u00e9 et clos\n        short_entries_indexes += 1\n        short_entries_indexes = short_entries_indexes.to_list()\n        \n        # Correction : Si la liste d'entr\u00e9e est vide -> donne 1 seule entr\u00e9e, celle de la date du jour\n        if not short_entries_indexes :\n            short_entries_indexes.append(max(df.index))\n        \n        # Attention, avec le +=1, si signal le dernier jour on est out of bounds de la liste. Correction\n        if short_entries_indexes[-1] > max(df.index):\n            short_entries_indexes[-1] = max(df.index)\n\n        # Application des deux fonctions pr\u00e9c\u00e9dentes pour recherche Date + Valeur, d'entr\u00e9e et sortie\n        for entry in short_entries_indexes:\n            # Calcul stop loss, take profit, et stocke Date + Open correspondants \u00e0 l'index d'entr\u00e9e pass\u00e9 dans la fonction\n            entry_date, entry_price, sl, tp = generate_exit_vars(df, entry, self.ATR_SL, self.RR_ratio)\n            # D\u00e9tetion de Date + Close de la sortie, identif\u00e9e avec Take Profit et stop loss trouv\u00e9s ligne pr\u00e9c\u00e9dente\n            exit_date, exit_price = return_SL_or_TP_index(df, entry, sl, tp)\n\n            # Ajout des r\u00e9sultats dans le dictionnaire\n            d_entrees_sorties[\"date_entree\"].append(entry_date)\n            d_entrees_sorties[\"valeur_entree\"].append(entry_price)\n            d_entrees_sorties[\"date_sortie\"].append(exit_date)\n            d_entrees_sorties[\"valeur_sortie\"].append(exit_price)\n\n        df_es = pd.DataFrame(d_entrees_sorties)\n        return df_es\n  \n\n    def performance_metrics_short(self, df_es):\n        ''' Sur la base du DataFrame Entr\u00e9es/Sorties, g\u00e9n\u00e8re les performances pour chaque trade, l'\u00e9quity curve base 1000,\n        le Win Rate, Expectancy Ratio, Expectancy, Profit Factor, Gain & Perte moyenne, et copie l'\u00e9tat des param\u00e8tres'''\n\n        df = df_es.copy()\n\n        try : \n            # Performance en % pour chaque trade pris (par ligne). Signe - car strat\u00e9gie Short\n            s_perf = round( -((df[\"valeur_sortie\"]-df[\"valeur_entree\"])/df[\"valeur_entree\"]), 3)\n            a_perf = np.array(s_perf)\n\n            # Win Rate\n            count_won = len( a_perf[a_perf > 0] )\n            count_lost = len(a_perf) - count_won\n            win_rate = round( count_won/len(a_perf) ,2)\n\n            # Reward-to-Risk Ratio X Win Ratio - Loss Ratio = Expectancy Ratio\n            expectancy_ratio = round( self.RR_ratio * count_won/len(a_perf) - count_lost/len(a_perf) ,2)\n\n            # Equity Curve, base 1000\n            l_perf = s_perf.to_list()\n            perf_nette_b1000 = [round( (999*(1+x*self.leverage)-1000)*0.99 ,2) for x in l_perf]\n\n            # Gain & perte moyenne (arrondi entier)\n            arr_b1000 = np.array(perf_nette_b1000)\n            moyenne_gains_nets = int( np.mean(arr_b1000[ arr_b1000>0 ]) )\n            moyenne_pertes_nettes = int( np.mean(arr_b1000[ arr_b1000<0 ]) )\n\n            # Expectancy (arrondi entier)\n            expectancy = int( (count_won/len(a_perf) * moyenne_gains_nets) - (count_lost/len(a_perf) * moyenne_pertes_nettes) )\n\n            # Profit Factor : sommes des gains / somme des pertes\n            profit_factor = round( np.sum(arr_b1000[ arr_b1000>0 ]) / np.sum( np.abs(arr_b1000[ arr_b1000<0 ]) ) ,2)\n            \n            # Agr\u00e9gation des r\u00e9sultats si DataFrame ES non nul\n            row = [win_rate, moyenne_gains_nets, moyenne_pertes_nettes, expectancy, expectancy_ratio, profit_factor, perf_nette_b1000]\n        \n        except:\n            # Si valeurs Nan ou autre g\u00e9n\u00e9rant une erreur dans les m\u00e9triques\n            row = [0, 0, 0, 0, 0, 0, [0]]\n\n        # Tous les attributs de l'object au moment du test\n        #d_attr = x.__dict__\n        d_attr = self.__dict__\n        row.append(d_attr)\n\n        return row\n    \n    \n    def apply_long_strategy(self, df_entries):\n        ''' Entr\u00e9e : DataFrame avec indicateurs techniques g\u00e9n\u00e9r\u00e9 par get_entries_signals\n        Sortie : DataFrame avec entr\u00e9e - sortie, valeur & date'''\n\n        df = df_entries.copy()\n        d_entrees_sorties = {\n            \"date_entree\" : [],\n            \"valeur_entree\" : [],\n            \"date_sortie\" : [],\n            \"valeur_sortie\" : []\n        }\n\n        # Liste des entr\u00e9es : +1 pour signal Long\n        short_entries_indexes = df.loc[ df[\"Entry\"]==1 ].index\n        # Ajout d'une unit\u00e9 pour entr\u00e9e le lendemain du signal confirm\u00e9 et clos\n        short_entries_indexes += 1\n        short_entries_indexes = short_entries_indexes.to_list()\n        \n        # Correction : Si la liste d'entr\u00e9e est vide -> donne 1 seule entr\u00e9e, celle de la date du jour\n        if not short_entries_indexes :\n            short_entries_indexes.append(max(df.index))\n        \n        # Correction : avec le +=1, si signal le dernier jour on est out of bounds de la liste\n        if short_entries_indexes[-1] > max(df.index) :\n            short_entries_indexes[-1] = max(df.index)\n\n        # Application des deux fonctions pr\u00e9c\u00e9dentes pour recherche Date + Valeur, d'entr\u00e9e et sortie\n        for entry in short_entries_indexes:\n            # Calcul stop loss, take profit, et stocke Date + Open correspondants \u00e0 l'index d'entr\u00e9e pass\u00e9 dans la fonction\n            entry_date, entry_price, sl, tp = generate_exit_vars_Long(df, entry, self.ATR_SL, self.RR_ratio)\n            # D\u00e9tetion de Date + Close de la sortie, identif\u00e9e avec Take Profit et stop loss trouv\u00e9s ligne pr\u00e9c\u00e9dente\n            exit_date, exit_price = return_SL_or_TP_index_Long(df, entry, sl, tp)\n\n            # Ajout des r\u00e9sultats dans le dictionnaire\n            d_entrees_sorties[\"date_entree\"].append(entry_date)\n            d_entrees_sorties[\"valeur_entree\"].append(entry_price)\n            d_entrees_sorties[\"date_sortie\"].append(exit_date)\n            d_entrees_sorties[\"valeur_sortie\"].append(exit_price)\n\n        df_es = pd.DataFrame(d_entrees_sorties)\n        return df_es\n    \n    \n    def performance_metrics_long(self, df_es):\n        ''' Sur la base du DataFrame Entr\u00e9es/Sorties, g\u00e9n\u00e8re les performances pour chaque trade, l'\u00e9quity curve base 1000,\n        le Win Rate, Expectancy Ratio, Expectancy, Profit Factor, Gain & Perte moyenne, et copie l'\u00e9tat des param\u00e8tres'''\n\n        df = df_es.copy()\n\n        try :\n            # Performance en % pour chaque trade pris (par ligne). Signe - car strat\u00e9gie Short\n            s_perf = round( ((df[\"valeur_sortie\"]-df[\"valeur_entree\"])/df[\"valeur_entree\"]), 3)\n            a_perf = np.array(s_perf)\n\n            # Win Rate\n            count_won = len( a_perf[a_perf > 0] )\n            count_lost = len(a_perf) - count_won\n            win_rate = round( count_won/len(a_perf) ,2)\n\n            # Reward-to-Risk Ratio X Win Ratio - Loss Ratio = Expectancy Ratio\n            expectancy_ratio = round( self.RR_ratio * count_won/len(a_perf) - count_lost/len(a_perf) ,2)\n\n            # Equity Curve, base 1000\n            l_perf = s_perf.to_list()\n            # Commission de 1% en entr\u00e9e et en sortie\n            perf_nette_b1000 = [round( (999*(1+x*self.leverage)-1000)*0.99 ,2) for x in l_perf]\n\n            # Gain & perte moyenne (arrondi entier)\n            arr_b1000 = np.array(perf_nette_b1000)\n            moyenne_gains_nets = int( np.mean(arr_b1000[ arr_b1000>0 ]) )\n            moyenne_pertes_nettes = int( np.mean(arr_b1000[ arr_b1000<0 ]) )\n\n            # Expectancy (arrondi entier)\n            expectancy = int( (count_won/len(a_perf) * moyenne_gains_nets) - (count_lost/len(a_perf) * moyenne_pertes_nettes) )\n\n            # Profit Factor : sommes des gains / somme des pertes\n            profit_factor = round( np.sum(arr_b1000[ arr_b1000>0 ]) / np.sum( np.abs(arr_b1000[ arr_b1000<0 ]) ) ,2)\n            \n            # Agr\u00e9gation des r\u00e9sultats si DataFrame ES non nul\n            row = [win_rate, moyenne_gains_nets, moyenne_pertes_nettes, expectancy, expectancy_ratio, profit_factor, perf_nette_b1000]\n        \n        except:\n            # Si valeurs Nan ou autre g\u00e9n\u00e9rant une erreur dans les m\u00e9triques\n            row = [0, 0, 0, 0, 0, 0, [0]]\n\n        # Tous les attributs de l'object au moment du test\n        #d_attr = x.__dict__\n        d_attr = self.__dict__\n        row.append(d_attr)\n\n        return row\n    ", "execution_count": 17, "outputs": []}, {"metadata": {"id": "4waqtIuFF05S"}, "cell_type": "markdown", "source": "# Optimisation Bayesienne"}, {"metadata": {"id": "e1dc2f1fc48345318cd94f63a5939aee"}, "cell_type": "markdown", "source": "NOTE : Si passage Long <-> Short, ajuster la parama\u00e8tre .stc_seuil_x <br/>\nAinsi que la m\u00e9thode .apply_x_strategy( ) et .performance_metrics_x( )"}, {"metadata": {"id": "388a8cad47054643acc502bb189957a1"}, "cell_type": "code", "source": "def best_Short(p_ema, p_AT_m, p_AT_l, p_AT_mfi_l, p_AT_mfi_s, p_STC_l, p_STC_slow_l, p_STC_fast_l, p_STC_h, p_ATR_SL_l, p_ATR_SL, p_RR_ratio):\n    '''Meilleur Profit Factor pour strat\u00e9gie Long.\n    Tous param\u00e8tres sauf Leverage'''\n    try :\n        # nouvel objet\n        bayes = Strat_AT_STC_EMA()\n\n        # Attribution des param\u00e8tres\n        bayes.ema_l = int(p_ema)\n        bayes.at_m = round(p_AT_m ,1)\n        bayes.at_l = int(p_AT_l)\n        bayes.at_mfi_l = int(p_AT_mfi_l)\n        bayes.at_mfi_s =  int(p_AT_mfi_s)\n        bayes.stc_l = int(p_STC_l)\n        bayes.stc_s_l = int(p_STC_slow_l)\n        bayes.stc_f_l = int(p_STC_fast_l)\n        bayes.stc_seuil_h = int(p_STC_h)\n        bayes.ATR_SL_l = int(p_ATR_SL_l)\n        bayes.ATR_SL = round(p_ATR_SL, 1)\n        bayes.RR_ratio = round(p_RR_ratio, 1)\n   \n        # Calculs\n        df_indicateurs_techniques = bayes.make_technical_indicators(df_historical_data)\n        df_signaux = bayes.get_entries_signals(df_indicateurs_techniques)\n        df_entrees_sorties = bayes.apply_short_strategy(df_signaux)\n\n        ligne_resultat = bayes.performance_metrics_short(df_entrees_sorties)\n        # Profit Factor\n        pf = ligne_resultat[5]\n        # Expected Gain\n        eg = len(ligne_resultat[6]) * ligne_resultat[3]        \n                \n        #return pf\n        return eg\n    \n    except Exception:\n        logging.exception(\"f(%r) failed\" % (args,))", "execution_count": 18, "outputs": []}, {"metadata": {"id": "b5dca1c7c0c6482d8a2f5c306dac6c54"}, "cell_type": "code", "source": "# Bounded region of parameter space\npbounds = {'p_ema' : (20,130), \n           'p_AT_m' : (0.2,1.6), \n           'p_AT_l' : (6,40), \n           'p_AT_mfi_l' : (10,20), \n           'p_AT_mfi_s' : (50,58), \n           'p_STC_l' : (50,140), \n           'p_STC_slow_l' : (40,130), \n           'p_STC_fast_l' : (10,40), \n           'p_STC_h' : (65,90), \n           'p_ATR_SL_l' : (6,40), \n           'p_ATR_SL' : (1,4), \n           'p_RR_ratio' : (2,6) \n           }", "execution_count": 20, "outputs": []}, {"metadata": {"id": "220549e0a031480c86ab515e067d17ca"}, "cell_type": "markdown", "source": "#### Test fonctionnement opitimisation\nToutes options par d\u00e9faut"}, {"metadata": {"id": "041f663748b84a43805909a19eb94af6"}, "cell_type": "code", "source": "optimizer = BayesianOptimization(\n    f=best_Short,\n    pbounds=pbounds,\n    random_state=1,\n    verbose=1\n)", "execution_count": 21, "outputs": []}, {"metadata": {"id": "449a271ef4124a7d89f1dffc1897bd91"}, "cell_type": "code", "source": "optimizer.maximize(\n    init_points=10,\n    n_iter=100,\n)", "execution_count": 22, "outputs": [{"name": "stdout", "text": "|   iter    |  target   | p_ATR_SL  | p_ATR_... |  p_AT_l   |  p_AT_m   | p_AT_m... | p_AT_m... | p_RR_r... | p_STC_... |  p_STC_h  |  p_STC_l  | p_STC_... |   p_ema   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| \u001b[95m3        \u001b[0m | \u001b[95m3.179e+03\u001b[0m | \u001b[95m3.629    \u001b[0m | \u001b[95m36.42    \u001b[0m | \u001b[95m8.892    \u001b[0m | \u001b[95m0.2547   \u001b[0m | \u001b[95m11.7     \u001b[0m | \u001b[95m57.03    \u001b[0m | \u001b[95m2.393    \u001b[0m | \u001b[95m22.63    \u001b[0m | \u001b[95m88.95    \u001b[0m | \u001b[95m97.98    \u001b[0m | \u001b[95m102.3    \u001b[0m | \u001b[95m54.71    \u001b[0m |\n| \u001b[95m4        \u001b[0m | \u001b[95m3.638e+03\u001b[0m | \u001b[95m3.06     \u001b[0m | \u001b[95m34.38    \u001b[0m | \u001b[95m6.622    \u001b[0m | \u001b[95m1.25     \u001b[0m | \u001b[95m19.89    \u001b[0m | \u001b[95m55.99    \u001b[0m | \u001b[95m3.122    \u001b[0m | \u001b[95m33.68    \u001b[0m | \u001b[95m67.58    \u001b[0m | \u001b[95m90.31    \u001b[0m | \u001b[95m121.8    \u001b[0m | \u001b[95m52.3     \u001b[0m |\n| \u001b[95m10       \u001b[0m | \u001b[95m4.07e+03 \u001b[0m | \u001b[95m3.073    \u001b[0m | \u001b[95m39.91    \u001b[0m | \u001b[95m11.86    \u001b[0m | \u001b[95m0.392    \u001b[0m | \u001b[95m19.33    \u001b[0m | \u001b[95m55.57    \u001b[0m | \u001b[95m2.264    \u001b[0m | \u001b[95m32.66    \u001b[0m | \u001b[95m83.85    \u001b[0m | \u001b[95m133.1    \u001b[0m | \u001b[95m104.0    \u001b[0m | \u001b[95m33.67    \u001b[0m |\n| \u001b[95m18       \u001b[0m | \u001b[95m5.445e+03\u001b[0m | \u001b[95m2.727    \u001b[0m | \u001b[95m36.76    \u001b[0m | \u001b[95m12.52    \u001b[0m | \u001b[95m0.3107   \u001b[0m | \u001b[95m19.28    \u001b[0m | \u001b[95m54.66    \u001b[0m | \u001b[95m3.215    \u001b[0m | \u001b[95m33.59    \u001b[0m | \u001b[95m77.9     \u001b[0m | \u001b[95m130.4    \u001b[0m | \u001b[95m102.3    \u001b[0m | \u001b[95m32.83    \u001b[0m |\n| \u001b[95m23       \u001b[0m | \u001b[95m6e+03    \u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m34.38    \u001b[0m | \u001b[95m12.9     \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m19.99    \u001b[0m | \u001b[95m56.07    \u001b[0m | \u001b[95m2.043    \u001b[0m | \u001b[95m34.72    \u001b[0m | \u001b[95m75.91    \u001b[0m | \u001b[95m129.2    \u001b[0m | \u001b[95m103.7    \u001b[0m | \u001b[95m32.39    \u001b[0m |\n| \u001b[95m27       \u001b[0m | \u001b[95m6.318e+03\u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m34.96    \u001b[0m | \u001b[95m11.62    \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m18.73    \u001b[0m | \u001b[95m54.27    \u001b[0m | \u001b[95m2.854    \u001b[0m | \u001b[95m34.75    \u001b[0m | \u001b[95m76.16    \u001b[0m | \u001b[95m128.9    \u001b[0m | \u001b[95m102.5    \u001b[0m | \u001b[95m30.35    \u001b[0m |\n| \u001b[95m52       \u001b[0m | \u001b[95m6.38e+03 \u001b[0m | \u001b[95m3.883    \u001b[0m | \u001b[95m31.86    \u001b[0m | \u001b[95m7.308    \u001b[0m | \u001b[95m0.3309   \u001b[0m | \u001b[95m18.91    \u001b[0m | \u001b[95m54.39    \u001b[0m | \u001b[95m5.142    \u001b[0m | \u001b[95m29.66    \u001b[0m | \u001b[95m71.73    \u001b[0m | \u001b[95m127.0    \u001b[0m | \u001b[95m107.0    \u001b[0m | \u001b[95m31.07    \u001b[0m |\n| \u001b[95m61       \u001b[0m | \u001b[95m7.482e+03\u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m21.14    \u001b[0m | \u001b[95m10.07    \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m16.64    \u001b[0m | \u001b[95m50.0     \u001b[0m | \u001b[95m2.09     \u001b[0m | \u001b[95m19.0     \u001b[0m | \u001b[95m65.24    \u001b[0m | \u001b[95m126.2    \u001b[0m | \u001b[95m77.57    \u001b[0m | \u001b[95m31.69    \u001b[0m |\n| \u001b[95m64       \u001b[0m | \u001b[95m8.125e+03\u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m18.98    \u001b[0m | \u001b[95m7.85     \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m14.43    \u001b[0m | \u001b[95m53.77    \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m19.35    \u001b[0m | \u001b[95m65.0     \u001b[0m | \u001b[95m128.7    \u001b[0m | \u001b[95m77.42    \u001b[0m | \u001b[95m31.33    \u001b[0m |\n| \u001b[95m66       \u001b[0m | \u001b[95m8.296e+03\u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m18.59    \u001b[0m | \u001b[95m6.0      \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m13.97    \u001b[0m | \u001b[95m51.55    \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m21.9     \u001b[0m | \u001b[95m65.0     \u001b[0m | \u001b[95m126.0    \u001b[0m | \u001b[95m79.12    \u001b[0m | \u001b[95m27.3     \u001b[0m |\n| \u001b[95m81       \u001b[0m | \u001b[95m9.126e+03\u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m27.89    \u001b[0m | \u001b[95m6.0      \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m50.0     \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m29.51    \u001b[0m | \u001b[95m65.74    \u001b[0m | \u001b[95m138.9    \u001b[0m | \u001b[95m61.86    \u001b[0m | \u001b[95m26.58    \u001b[0m |\n| \u001b[95m82       \u001b[0m | \u001b[95m1.008e+04\u001b[0m | \u001b[95m4.0      \u001b[0m | \u001b[95m27.6     \u001b[0m | \u001b[95m6.0      \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m50.0     \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m27.2     \u001b[0m | \u001b[95m66.36    \u001b[0m | \u001b[95m140.0    \u001b[0m | \u001b[95m59.49    \u001b[0m | \u001b[95m20.0     \u001b[0m |\n=========================================================================================================================================================================\n", "output_type": "stream"}]}, {"metadata": {"id": "897dc76f95ae4e62b382ef775ce7f74a"}, "cell_type": "code", "source": "print(optimizer.max)", "execution_count": 23, "outputs": [{"name": "stdout", "text": "{'target': 10080.0, 'params': {'p_ATR_SL': 4.0, 'p_ATR_SL_l': 27.60487148541571, 'p_AT_l': 6.0, 'p_AT_m': 0.2, 'p_AT_mfi_l': 10.0, 'p_AT_mfi_s': 50.0, 'p_RR_ratio': 2.0, 'p_STC_fast_l': 27.20301613597171, 'p_STC_h': 66.35691229167223, 'p_STC_l': 140.0, 'p_STC_slow_l': 59.49183400908253, 'p_ema': 20.0}}\n", "output_type": "stream"}]}, {"metadata": {"id": "b5424860269445cb8b4bf35836f26bd9"}, "cell_type": "markdown", "source": "## Optimisations par fonction d'acquisiton"}, {"metadata": {"id": "010c03439a8841e0838309c76611beb2"}, "cell_type": "code", "source": "from bayes_opt import UtilityFunction", "execution_count": 24, "outputs": []}, {"metadata": {"id": "bff299b313d94dfb98049a4c19881606"}, "cell_type": "code", "source": "help(optimizer.space)", "execution_count": 25, "outputs": [{"name": "stdout", "text": "Help on TargetSpace in module bayes_opt.target_space object:\n\nclass TargetSpace(builtins.object)\n |  TargetSpace(target_func, pbounds, constraint=None, random_state=None, allow_duplicate_points=False)\n |  \n |  Holds the param-space coordinates (X) and target values (Y)\n |  Allows for constant-time appends while ensuring no duplicates are added\n |  \n |  Example\n |  -------\n |  >>> def target_func(p1, p2):\n |  >>>     return p1 + p2\n |  >>> pbounds = {'p1': (0, 1), 'p2': (1, 100)}\n |  >>> space = TargetSpace(target_func, pbounds, random_state=0)\n |  >>> x = space.random_points(1)[0]\n |  >>> y = space.register_point(x)\n |  >>> assert self.max_point()['max_val'] == y\n |  \n |  Methods defined here:\n |  \n |  __contains__(self, x)\n |  \n |  __init__(self, target_func, pbounds, constraint=None, random_state=None, allow_duplicate_points=False)\n |      Parameters\n |      ----------\n |      target_func : function\n |          Function to be maximized.\n |      \n |      pbounds : dict\n |          Dictionary with parameters names as keys and a tuple with minimum\n |          and maximum values.\n |      \n |      random_state : int, RandomState, or None\n |          optionally specify a seed for a random number generator\n |      \n |      allow_duplicate_points: bool, optional (default=False)\n |          If True, the optimizer will allow duplicate points to be registered.\n |          This behavior may be desired in high noise situations where repeatedly probing\n |          the same point will give different answers. In other situations, the acquisition\n |          may occasionaly generate a duplicate point.\n |  \n |  __len__(self)\n |  \n |  array_to_params(self, x)\n |  \n |  max(self)\n |      Get maximum target value found and corresponding parameters.\n |      \n |      If there is a constraint present, the maximum value that fulfills the\n |      constraint is returned.\n |  \n |  params_to_array(self, params)\n |  \n |  probe(self, params)\n |      Evaulates a single point x, to obtain the value y and then records them\n |      as observations.\n |      \n |      Notes\n |      -----\n |      If x has been previously seen returns a cached value of y.\n |      \n |      Parameters\n |      ----------\n |      x : ndarray\n |          a single point, with len(x) == self.dim\n |      \n |      Returns\n |      -------\n |      y : float\n |          target function value.\n |  \n |  random_sample(self)\n |      Creates random points within the bounds of the space.\n |      \n |      Returns\n |      ----------\n |      data: ndarray\n |          [num x dim] array points with dimensions corresponding to `self._keys`\n |      \n |      Example\n |      -------\n |      >>> target_func = lambda p1, p2: p1 + p2\n |      >>> pbounds = {'p1': (0, 1), 'p2': (1, 100)}\n |      >>> space = TargetSpace(target_func, pbounds, random_state=0)\n |      >>> space.random_points(1)\n |      array([[ 55.33253689,   0.54488318]])\n |  \n |  register(self, params, target, constraint_value=None)\n |      Append a point and its target value to the known data.\n |      \n |      Parameters\n |      ----------\n |      x : ndarray\n |          a single point, with len(x) == self.dim\n |      \n |      y : float\n |          target function value\n |      \n |      Raises\n |      ------\n |      KeyError:\n |          if the point is not unique\n |      \n |      Notes\n |      -----\n |      runs in ammortized constant time\n |      \n |      Example\n |      -------\n |      >>> pbounds = {'p1': (0, 1), 'p2': (1, 100)}\n |      >>> space = TargetSpace(lambda p1, p2: p1 + p2, pbounds)\n |      >>> len(space)\n |      0\n |      >>> x = np.array([0, 0])\n |      >>> y = 1\n |      >>> space.add_observation(x, y)\n |      >>> len(space)\n |      1\n |  \n |  res(self)\n |      Get all target values and constraint fulfillment for all parameters.\n |  \n |  set_bounds(self, new_bounds)\n |      A method that allows changing the lower and upper searching bounds\n |      \n |      Parameters\n |      ----------\n |      new_bounds : dict\n |          A dictionary with the parameter name and its new bounds\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  bounds\n |  \n |  constraint\n |  \n |  constraint_values\n |  \n |  dim\n |  \n |  empty\n |  \n |  keys\n |  \n |  params\n |  \n |  target\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n", "output_type": "stream"}]}, {"metadata": {"id": "62594b707b87401f8d2143d13d425938"}, "cell_type": "markdown", "source": "<b> Description des fonctions d'optimisation </b>\n<p>Choosing the most suitable acquisition function depends on the specific characteristics of your optimization problem. <br>Given that you have an enormous number of combinations (over a trillion), exploration is likely crucial in the initial stages to find promising regions. <br>Therefore, starting with EI or UCB could be beneficial as they inherently balance exploration and exploitation.</p>\n\n<p>EI, being a well-rounded acquisition function, is often a popular choice. <br>It performs well in a variety of scenarios, striking a balance between exploration and exploitation. <br>If your objective function is relatively smooth and free from many local optima, EI might be a good starting point.</p>\n\n<p>UCB, on the other hand, can be more aggressive in exploring uncertain regions. <br>If your objective function is highly noisy or has a complex landscape with <b>many local optima</b>, UCB might help in discovering diverse regions and avoiding premature convergence.</p>"}, {"metadata": {"id": "db67d4788b25484b85556d680c8b0769"}, "cell_type": "markdown", "source": "### Par d\u00e9faut"}, {"metadata": {"id": "7fc5b03ff99d462981d4d0c04a8630f3"}, "cell_type": "code", "source": "optimizer_expl = BayesianOptimization(\n    f = best_Short,\n    pbounds = pbounds,\n    random_state = 11,\n    verbose = 1,\n)", "execution_count": 27, "outputs": []}, {"metadata": {"id": "7fc5b03ff99d462981d4d0c04a8630f3"}, "cell_type": "code", "source": "optimizer_expl.maximize(\n    init_points = 50,\n    n_iter = 1000,\n)", "execution_count": 27, "outputs": []}, {"metadata": {"id": "26778c91e569488b91a8d25029575de9"}, "cell_type": "code", "source": "print(optimizer_expl.max)", "execution_count": 28, "outputs": [{"name": "stdout", "text": "{'target': 11058.0, 'params': {'p_ATR_SL': 4.0, 'p_ATR_SL_l': 6.0, 'p_AT_l': 6.0, 'p_AT_m': 0.2, 'p_AT_mfi_l': 10.0, 'p_AT_mfi_s': 50.0, 'p_RR_ratio': 2.0, 'p_STC_fast_l': 40.0, 'p_STC_h': 65.0, 'p_STC_l': 108.92170266099272, 'p_STC_slow_l': 130.0, 'p_ema': 20.0}}\n", "output_type": "stream"}]}, {"metadata": {"id": "9b76fc2b6c004ce0a80a19fb0425c31e"}, "cell_type": "code", "source": "l_res_default = optimizer_expl.space.res()", "execution_count": 29, "outputs": []}, {"metadata": {"id": "61e8719835ee49e084cafcaa70a563fe"}, "cell_type": "code", "source": "df_default = pd.DataFrame(l_res_default, columns=[\"target\",\"params\"])\\\n            .sort_values(by=[\"target\"], ascending=False)\\\n            .reset_index(drop=True)", "execution_count": 31, "outputs": []}, {"metadata": {"id": "61e8719835ee49e084cafcaa70a563fe"}, "cell_type": "code", "source": "df_default.head(1)", "execution_count": 31, "outputs": []}, {"metadata": {"id": "154326e3e29b433481a9675ce4d41b58"}, "cell_type": "markdown", "source": "### Upper Confidence Bound"}, {"metadata": {"id": "eead5216448942be899054f185f5a3da"}, "cell_type": "markdown", "source": "UCB/EXPLORATION"}, {"metadata": {"id": "94ecb0d0a78345b4bbea40a8a481aa14"}, "cell_type": "code", "source": "optimizer_UCB_exploration = BayesianOptimization(\n    f = best_Short,\n    pbounds = pbounds,\n    random_state = 2020,\n    verbose = 1,\n)", "execution_count": null, "outputs": []}, {"metadata": {"id": "aca3e36408a14b4c8b98c37474090fc5"}, "cell_type": "code", "source": "'''Common Range: 1e-6 to 1e-2\nExplanation: The alpha parameter controls the amount of noise in the observed data. \nHigher values indicate higher noise levels, while lower values indicate less noise.'''\noptimizer_UCB_exploration.set_gp_params(alpha=1e-3, kernel=None, n_restarts_optimizer=5)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d818fa47d1db4b8eb33b6192c6192272"}, "cell_type": "code", "source": "# Valeur Kappa 10 = maximum Exploration \n# (0.1 = maximum Exploitation)\nacquisition_function = UtilityFunction(kind=\"ucb\", kappa=5)", "execution_count": null, "outputs": []}, {"metadata": {"id": "a69ab007f5404545be5b88ebb1ed0b80"}, "cell_type": "code", "source": "optimizer_UCB_exploration.maximize(\n    init_points = 50,\n    n_iter = 1000,\n    acquisition_function=acquisition_function,\n)", "execution_count": 42, "outputs": []}, {"metadata": {"id": "fc21fe732a5d44bc8116a9d962c115d6"}, "cell_type": "code", "source": "l_res_ucb_exploration = optimizer_UCB_exploration.space.res()", "execution_count": null, "outputs": []}, {"metadata": {"id": "935057aad9644b2081d744af69d8963e"}, "cell_type": "code", "source": "df_ucb_exploration = pd.DataFrame(l_res_ucb_exploration, columns=[\"target\",\"params\"])\\\n            .sort_values(by=[\"target\"], ascending=False)\\\n            .reset_index(drop=True)", "execution_count": null, "outputs": []}, {"metadata": {"id": "935057aad9644b2081d744af69d8963e"}, "cell_type": "code", "source": "df_ucb_exploration.head(1)", "execution_count": null, "outputs": []}, {"metadata": {"id": "eead5216448942be899054f185f5a3da"}, "cell_type": "markdown", "source": "UCB /EXPLOITATION"}, {"metadata": {"id": "94ecb0d0a78345b4bbea40a8a481aa14"}, "cell_type": "code", "source": "optimizer_UCB_exploitation = BayesianOptimization(\n    f = best_Short,\n    pbounds = pbounds,\n    random_state = 51,\n    verbose = 1,\n    allow_duplicate_points = True,\n)", "execution_count": null, "outputs": []}, {"metadata": {"id": "aca3e36408a14b4c8b98c37474090fc5"}, "cell_type": "code", "source": "'''Common Range: 1e-6 to 1e-2\nExplanation: The alpha parameter controls the amount of noise in the observed data. \nHigher values indicate higher noise levels, while lower values indicate less noise.'''\noptimizer_UCB_exploitation.set_gp_params(alpha=1e-4, kernel=None, n_restarts_optimizer=5)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d818fa47d1db4b8eb33b6192c6192272"}, "cell_type": "code", "source": "# Valeur Kappa : 10 = maximum Exploration \n# 0.1 = maximum Exploitation\nacquisition_function = UtilityFunction(kind=\"ucb\", kappa=0.5)", "execution_count": null, "outputs": []}, {"metadata": {"id": "a69ab007f5404545be5b88ebb1ed0b80"}, "cell_type": "code", "source": "st1 = time.time()\n\noptimizer_UCB_exploitation.maximize(\n    init_points = 100,\n    n_iter = 1000,\n    acquisition_function = acquisition_function,\n)\n\net1 = time.time()", "execution_count": 42, "outputs": []}, {"metadata": {"id": "78d599ad9ee341c09bf8610496d6c527"}, "cell_type": "code", "source": "elapsed_time = (et1 - st1)/60\nprint('Execution time: {} minutes'.format(int(elapsed_time)))", "execution_count": null, "outputs": []}, {"metadata": {"id": "fc21fe732a5d44bc8116a9d962c115d6"}, "cell_type": "code", "source": "l_res_ucb_exploitation = optimizer_UCB_exploitation.space.res()", "execution_count": null, "outputs": []}, {"metadata": {"id": "935057aad9644b2081d744af69d8963e"}, "cell_type": "code", "source": "df_ucb_exploitation = pd.DataFrame(l_res_ucb_exploitation, columns=[\"target\",\"params\"])\\\n            .sort_values(by=[\"target\"], ascending=False)\\\n            .reset_index(drop=True)", "execution_count": null, "outputs": []}, {"metadata": {"id": "935057aad9644b2081d744af69d8963e"}, "cell_type": "code", "source": "df_ucb_exploitation.head(1)", "execution_count": null, "outputs": []}, {"metadata": {"id": "926990daad9c49368ee17b61db82a55a"}, "cell_type": "code", "source": "df_ucb = pd.concat([df_ucb_exploration, df_ucb_exploitation])\ndf_ucb = pd.concat([df_ucb, df_ucb_exploitation, df_default])", "execution_count": 47, "outputs": []}, {"metadata": {"id": "88d32619289a49bf8349cc51a63f7d4d"}, "cell_type": "markdown", "source": "### Probability of Improvement"}, {"metadata": {"id": "70150be9563a45258a7bfcef7f6c0f39"}, "cell_type": "markdown", "source": "POI/EXPLOITATION"}, {"metadata": {"id": "4d0d975adb0a4a7d83aa690470bdcb85"}, "cell_type": "code", "source": "optimizer_POI_exploitation = BayesianOptimization(\n    f = best_Short,\n    pbounds = pbounds,\n    random_state = 64,\n    verbose = 1,\n    allow_duplicate_points = True,\n)", "execution_count": 48, "outputs": []}, {"metadata": {"id": "69c86b6a351646bf85e9deca17bcc035"}, "cell_type": "code", "source": "# exploration xi=0.1\n# exploitation xi=1e-4\nacquisition_function = UtilityFunction(kind=\"poi\", xi=1e-3)", "execution_count": 51, "outputs": []}, {"metadata": {"id": "fca4af7148a7462f90fad45ed06ec7af"}, "cell_type": "code", "source": "optimizer_POI_exploitation.set_gp_params(alpha=1e-4, kernel=None, n_restarts_optimizer=5)", "execution_count": 50, "outputs": []}, {"metadata": {"id": "69c86b6a351646bf85e9deca17bcc035"}, "cell_type": "code", "source": "optimizer_POI_exploitation.maximize(\n    init_points=100,\n    n_iter=1000,\n    acquisition_function=acquisition_function\n)", "execution_count": 51, "outputs": []}, {"metadata": {"id": "65e8b3f660a74e278b8f957dd159f58d"}, "cell_type": "code", "source": "# Obtention des r\u00e9sultats des it\u00e9rations\nl_poi_exploitation = optimizer_POI_exploitation.space.res()\n\n# Conversion en df + tri ordre d\u00e9croissant\ndf_poi_exploitation = pd.DataFrame(l_poi_exploitation, columns=[\"target\",\"params\"])\\\n            .sort_values(by=[\"target\"], ascending=False)\\\n            .reset_index(drop=True)", "execution_count": 53, "outputs": []}, {"metadata": {"id": "65e8b3f660a74e278b8f957dd159f58d"}, "cell_type": "code", "source": "df_poi_exploitation.head(1)", "execution_count": 53, "outputs": []}, {"metadata": {"id": "165fa384be8642eb95ce7fe341fe32f1"}, "cell_type": "markdown", "source": "POI/EXPLORATION"}, {"metadata": {"id": "cf3675ee98384677877751a6878d816a"}, "cell_type": "code", "source": "optimizer_POI_exploration = BayesianOptimization(\n    f = best_Short,\n    pbounds = pbounds,\n    random_state = 1664,\n    verbose = 1,\n    allow_duplicate_points = True,\n)", "execution_count": 54, "outputs": []}, {"metadata": {"id": "27a71c1af5f04e0b86bec77b8c9b80f5"}, "cell_type": "code", "source": "optimizer_POI_exploration.set_gp_params(alpha=1e-5, kernel=None, n_restarts_optimizer=5)", "execution_count": 55, "outputs": []}, {"metadata": {"id": "391b4b5b73924db590a34bcc0cc30d1d"}, "cell_type": "code", "source": "# Prefer exploration (xi=0.1)\n# exploitation xi=1e-4\nacquisition_function_poi = UtilityFunction(kind=\"poi\", xi=0.01)", "execution_count": 56, "outputs": []}, {"metadata": {"id": "ab28574673e54d9ea415181287da6cac"}, "cell_type": "code", "source": "st2 = time.time()\n\noptimizer_POI_exploration.maximize(\n    init_points = 50,\n    n_iter = 1500,\n    acquisition_function = acquisition_function_poi,\n)\n\net2 = time.time()", "execution_count": 57, "outputs": [{"name": "stdout", "text": "|   iter    |  target   | p_ATR_SL  | p_ATR_... |  p_AT_l   |  p_AT_m   | p_AT_m... | p_AT_m... | p_RR_r... | p_STC_... |  p_STC_h  |  p_STC_l  | p_STC_... |   p_ema   |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "output_type": "stream"}, {"name": "stderr", "text": "/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n", "output_type": "stream"}, {"name": "stdout", "text": "| \u001b[95m5        \u001b[0m | \u001b[95m3.4e+03  \u001b[0m | \u001b[95m3.236    \u001b[0m | \u001b[95m39.17    \u001b[0m | \u001b[95m17.69    \u001b[0m | \u001b[95m1.017    \u001b[0m | \u001b[95m18.87    \u001b[0m | \u001b[95m51.02    \u001b[0m | \u001b[95m4.368    \u001b[0m | \u001b[95m37.99    \u001b[0m | \u001b[95m74.53    \u001b[0m | \u001b[95m67.26    \u001b[0m | \u001b[95m59.55    \u001b[0m | \u001b[95m29.65    \u001b[0m |\n| \u001b[95m18       \u001b[0m | \u001b[95m4.158e+03\u001b[0m | \u001b[95m3.905    \u001b[0m | \u001b[95m23.03    \u001b[0m | \u001b[95m8.088    \u001b[0m | \u001b[95m0.6256   \u001b[0m | \u001b[95m12.82    \u001b[0m | \u001b[95m51.75    \u001b[0m | \u001b[95m2.64     \u001b[0m | \u001b[95m22.95    \u001b[0m | \u001b[95m83.69    \u001b[0m | \u001b[95m54.46    \u001b[0m | \u001b[95m66.76    \u001b[0m | \u001b[95m70.55    \u001b[0m |\n\u001b[91mData point [ 3.90520599 23.03423039  8.0880306   0.62555917 12.81880656 51.74794844\n  2.63960736 22.94516748 83.69345888 54.46099674 66.75993928 70.55040753] is not unique. 1 duplicates registered. Continuing ...\u001b[0m\n| \u001b[95m59       \u001b[0m | \u001b[95m5.136e+03\u001b[0m | \u001b[95m3.647    \u001b[0m | \u001b[95m16.65    \u001b[0m | \u001b[95m18.11    \u001b[0m | \u001b[95m0.6291   \u001b[0m | \u001b[95m17.54    \u001b[0m | \u001b[95m50.51    \u001b[0m | \u001b[95m4.724    \u001b[0m | \u001b[95m17.18    \u001b[0m | \u001b[95m68.81    \u001b[0m | \u001b[95m93.07    \u001b[0m | \u001b[95m127.0    \u001b[0m | \u001b[95m29.71    \u001b[0m |\n| \u001b[95m101      \u001b[0m | \u001b[95m6.327e+03\u001b[0m | \u001b[95m3.233    \u001b[0m | \u001b[95m39.71    \u001b[0m | \u001b[95m11.43    \u001b[0m | \u001b[95m0.4358   \u001b[0m | \u001b[95m10.5     \u001b[0m | \u001b[95m50.23    \u001b[0m | \u001b[95m3.155    \u001b[0m | \u001b[95m35.73    \u001b[0m | \u001b[95m74.02    \u001b[0m | \u001b[95m69.86    \u001b[0m | \u001b[95m60.45    \u001b[0m | \u001b[95m25.27    \u001b[0m |\n", "output_type": "stream"}, {"name": "stderr", "text": "/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n", "output_type": "stream"}, {"name": "stdout", "text": "| \u001b[95m519      \u001b[0m | \u001b[95m6.667e+03\u001b[0m | \u001b[95m3.029    \u001b[0m | \u001b[95m24.31    \u001b[0m | \u001b[95m12.71    \u001b[0m | \u001b[95m0.4499   \u001b[0m | \u001b[95m10.15    \u001b[0m | \u001b[95m55.04    \u001b[0m | \u001b[95m3.635    \u001b[0m | \u001b[95m28.14    \u001b[0m | \u001b[95m88.72    \u001b[0m | \u001b[95m83.17    \u001b[0m | \u001b[95m95.02    \u001b[0m | \u001b[95m22.18    \u001b[0m |\n| \u001b[95m526      \u001b[0m | \u001b[95m6.9e+03  \u001b[0m | \u001b[95m3.305    \u001b[0m | \u001b[95m25.15    \u001b[0m | \u001b[95m9.393    \u001b[0m | \u001b[95m0.3192   \u001b[0m | \u001b[95m10.43    \u001b[0m | \u001b[95m54.3     \u001b[0m | \u001b[95m2.859    \u001b[0m | \u001b[95m29.67    \u001b[0m | \u001b[95m80.93    \u001b[0m | \u001b[95m83.48    \u001b[0m | \u001b[95m84.59    \u001b[0m | \u001b[95m24.35    \u001b[0m |\n| \u001b[95m551      \u001b[0m | \u001b[95m7.875e+03\u001b[0m | \u001b[95m3.106    \u001b[0m | \u001b[95m27.67    \u001b[0m | \u001b[95m12.06    \u001b[0m | \u001b[95m0.2405   \u001b[0m | \u001b[95m13.28    \u001b[0m | \u001b[95m53.01    \u001b[0m | \u001b[95m3.172    \u001b[0m | \u001b[95m32.37    \u001b[0m | \u001b[95m74.6     \u001b[0m | \u001b[95m88.9     \u001b[0m | \u001b[95m96.93    \u001b[0m | \u001b[95m20.8     \u001b[0m |\n", "output_type": "stream"}, {"name": "stderr", "text": "/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\n/opt/conda/envs/Python-3.9/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n", "output_type": "stream"}, {"name": "stdout", "text": "| \u001b[95m1345     \u001b[0m | \u001b[95m8.05e+03 \u001b[0m | \u001b[95m3.95     \u001b[0m | \u001b[95m24.4     \u001b[0m | \u001b[95m8.432    \u001b[0m | \u001b[95m0.2094   \u001b[0m | \u001b[95m14.02    \u001b[0m | \u001b[95m54.62    \u001b[0m | \u001b[95m2.312    \u001b[0m | \u001b[95m35.68    \u001b[0m | \u001b[95m79.34    \u001b[0m | \u001b[95m87.14    \u001b[0m | \u001b[95m101.9    \u001b[0m | \u001b[95m22.11    \u001b[0m |\n=========================================================================================================================================================================\n", "output_type": "stream"}]}, {"metadata": {"id": "78d599ad9ee341c09bf8610496d6c527"}, "cell_type": "code", "source": "elapsed_time = (et2 - st2)/60\nprint('Execution time: {} minutes'.format(int(elapsed_time)))", "execution_count": null, "outputs": []}, {"metadata": {"id": "fc21fe732a5d44bc8116a9d962c115d6"}, "cell_type": "code", "source": "l_poi_exploration = optimizer_POI_exploration.space.res()", "execution_count": null, "outputs": []}, {"metadata": {"id": "935057aad9644b2081d744af69d8963e"}, "cell_type": "code", "source": "df_poi_exploration = pd.DataFrame(l_poi_exploration, columns=[\"target\",\"params\"])\\\n            .sort_values(by=[\"target\"], ascending=False)\\\n            .reset_index(drop=True)", "execution_count": null, "outputs": []}, {"metadata": {"id": "935057aad9644b2081d744af69d8963e"}, "cell_type": "code", "source": "df_poi_exploration.head(1)", "execution_count": null, "outputs": []}, {"metadata": {"id": "8ef379a215b34495b57b22516ec9b490"}, "cell_type": "markdown", "source": "Merge"}, {"metadata": {"id": "f62fdef39e87455b8a7ccc3ad5772470"}, "cell_type": "code", "source": "df_poi = pd.concat([df_poi_exploitation, df_poi_exploration])\ndf_bayes_concat = pd.concat([df_poi, df_ucb])", "execution_count": 62, "outputs": []}, {"metadata": {"id": "7141899653ab4d20b58aec925e763c00"}, "cell_type": "code", "source": "wslib = access_project_or_space()\nasset = wslib.save_data(\"Bayes_expl_Short_EG.csv\", data=str.encode(df_bayes_concat.to_csv(header=True, index=False)), overwrite=True)\nwslib.show(asset)", "execution_count": 63, "outputs": [{"name": "stdout", "text": "{'name': 'Bayes_expl_Short_EG.csv',\n 'asset_type': 'data_asset',\n 'asset_id': 'f157de49-2487-4a6b-b06a-cdab941fbfdd',\n 'attachment_id': 'a1497d7b-606e-4a52-8707-149490469f94',\n 'filepath': 'Bayes_expl_Short_EG.csv',\n 'data_size': None,\n 'mime': 'text/csv',\n 'summary': ['created or overwritten file',\n             'created data asset',\n             'created attachment']}\n", "output_type": "stream"}]}, {"metadata": {"id": "080c9090e270442a8272fc81977c22c6"}, "cell_type": "markdown", "source": "## Test Unitaire fonctions adapt\u00e9es Bayes"}, {"metadata": {"id": "1184c40f67f442319c646042c6a2a196"}, "cell_type": "code", "source": "st = time.time()\na, b = best_Long(p_ema=80, p_AT_m=0.2, p_AT_l=14, p_AT_mfi_l=12, p_AT_mfi_s=54, p_STC_l=100, p_STC_slow_l=90, p_STC_fast_l=15, p_STC_b=20, p_ATR_SL_l=36, p_ATR_SL=3, p_RR_ratio=3)\net = time.time()\nelapsed_time = (et - st)", "execution_count": null, "outputs": []}, {"metadata": {"id": "fda3751f1e7548009bd84cfb3796e2db"}, "cell_type": "code", "source": "print(\"Profit Factor : {}\\nExpected Gain : {}\\nDur\u00e9e : {:.2f}s\".format(a, b, elapsed_time))", "execution_count": null, "outputs": []}, {"metadata": {"id": "1b0d52ce4c7542429f168b815d7b8198"}, "cell_type": "markdown", "source": "## Performance d\u00e9taill\u00e9e sur la base des meilleures combinaisons de param\u00e8tres obtenus"}, {"metadata": {"id": "95975a5a4f5441a082a6105ea85e7087"}, "cell_type": "code", "source": "# Unnest de la colonne params\ndf_bayes_unnested = df_bayes_concat[\"params\"].apply(pd.Series)", "execution_count": 116, "outputs": []}, {"metadata": {"id": "7785b3b0d32244a79035b80493ae8cc8"}, "cell_type": "code", "source": "# jointure sur l'index pour retrouver \"target\"\ndf_bayes_unnested = df_bayes_concat.join(df_bayes_unnested)\n\n# suppression de l'ancienne colonne params au format json\ndf_bayes_unnested.drop(columns=[\"params\"],inplace=True)\n\ndf_bayes_unnested.sort_values(by=[\"target\"], ascending=False, inplace=True)", "execution_count": 117, "outputs": []}, {"metadata": {"id": "8771815bd41e4c3481920fba1f37be45"}, "cell_type": "code", "source": "# R\u00e9organisation des colonnes pour correspondre \u00e0 l'ordre de la classe\ndf_bayes_unnested = df_bayes_unnested[[\"target\",\"p_ema\",\"p_AT_l\",\"p_AT_m\",\"p_AT_mfi_l\",\"p_AT_mfi_s\",\"p_STC_l\",\"p_STC_slow_l\",\"p_STC_fast_l\",\"p_STC_h\",\"p_ATR_SL_l\",\"p_ATR_SL\",\"p_RR_ratio\"]]", "execution_count": 120, "outputs": []}, {"metadata": {"id": "1b0798c7dc7f4fa08d424b17f9b8501f"}, "cell_type": "code", "source": "# performances meilleur quantile, min 4194\ndf_bayes_unnested = df_bayes_unnested.loc[df_bayes_unnested[\"target\"]>4200]\ndf_bayes_unnested.shape", "execution_count": 122, "outputs": [{"output_type": "execute_result", "execution_count": 122, "data": {"text/plain": "(12438, 13)"}, "metadata": {}}]}, {"metadata": {"id": "d71250ba7e9c46a38065ec6f26110080"}, "cell_type": "code", "source": "l_params = []\n\nfor i in range(0, df_bayes_unnested.shape[0]):\n    # Conversion de la ligne en liste de param\u00e8tres\n    ligne = df_bayes_unnested.iloc[i,1:].to_list()\n    l_params.append(ligne)", "execution_count": 123, "outputs": []}, {"metadata": {"id": "8aab97a864a34121820edff262a4224b"}, "cell_type": "markdown", "source": "NOTE : Attention Long / Short + STC_seuil_h/b"}, {"metadata": {"id": "2c63533e41a9411b94383be3c7145558"}, "cell_type": "code", "source": "def mesure_performance_bayes(combinaison):\n    try : \n        # nouvel objet\n        bayes = Strat_AT_STC_EMA()\n\n        # Attribution des param\u00e8tres variables de test\n        bayes.ema_l = int(combinaison[0])\n        bayes.at_l = int(combinaison[1])\n        bayes.at_m = combinaison[2]\n        bayes.at_mfi_l = int(combinaison[3])\n        bayes.at_mfi_s = int(combinaison[4])\n        bayes.stc_l = int(combinaison[5])\n        bayes.stc_s_l = int(combinaison[6])\n        bayes.stc_f_l = int(combinaison[7])\n        bayes.stc_seuil_h = int(combinaison[8])\n        bayes.ATR_SL_l = int(combinaison[9])\n        bayes.ATR_SL = combinaison[10]\n        bayes.RR_ratio = combinaison[11]\n    \n        # Calculs\n        df_indicateurs_techniques = bayes.make_technical_indicators(df_historical_data)\n        df_signaux = bayes.get_entries_signals(df_indicateurs_techniques)\n        df_entrees_sorties = bayes.apply_short_strategy(df_signaux)\n\n        ligne_resultat = bayes.performance_metrics_short(df_entrees_sorties)\n        return ligne_resultat\n    \n    except Exception:\n        logging.exception(\"f(%r) failed\" % (args,))", "execution_count": 124, "outputs": []}, {"metadata": {"id": "22847efb5ffe43958f488824d1dba49b"}, "cell_type": "code", "source": "print(\"Number of processors: \", mp.cpu_count())", "execution_count": 125, "outputs": [{"output_type": "stream", "text": "Number of processors:  32\n", "name": "stdout"}]}, {"metadata": {"id": "5fff053c7ff149aab07d2e192bdc224f"}, "cell_type": "code", "source": "st = time.time()\n\nif __name__==\"__main__\":\n    pool = mp.Pool(mp.cpu_count())\n    resultat = pool.map(mesure_performance_bayes, [combi for combi in l_params])\n    pool.close()\n\net = time.time()", "execution_count": 127, "outputs": []}, {"metadata": {"id": "cac99be2098045348b2d67e774c01955"}, "cell_type": "code", "source": "elapsed_time = (et - st)\nprint(\"Temps \u00e9coul\u00e9 : {:.0f}mn pour {} lignes\".format(int(elapsed_time)/60, len(l_params)) )", "execution_count": 128, "outputs": [{"output_type": "stream", "text": "Temps \u00e9coul\u00e9 : 28mn pour 12438 lignes\n", "name": "stdout"}]}, {"metadata": {"id": "275e5f27dbd54ab4a87f932114404626"}, "cell_type": "code", "source": "df_perf_bayes = pd.DataFrame(resultat, columns = [\"win_rate\", \"moyenne_gains_nets\", \"moyenne_pertes_nettes\", \"expectancy\", \"expectancy_ratio\", \"profit_factor\", \"equity_curve\", \"parametres\"])\ndf_perf_bayes.shape", "execution_count": 129, "outputs": [{"output_type": "execute_result", "execution_count": 129, "data": {"text/plain": "(12438, 8)"}, "metadata": {}}]}, {"metadata": {"id": "1bcc6bc532fb468a84bb22099dc1690e"}, "cell_type": "code", "source": "# On \u00e9carte les trades suppos\u00e9s non profitables\ndf_perf_bayes_best_only = df_perf_bayes.loc[ df_perf_bayes[\"profit_factor\"]>2 ]", "execution_count": 130, "outputs": []}, {"metadata": {"id": "04b7f9258df2489289889c0b23df0c74"}, "cell_type": "code", "source": "def get_nb_trades(liste_trades):\n    '''d\u00e9duit le nombre de trades de la longueur de la liste equity_curve'''\n    return len(liste_trades)", "execution_count": 131, "outputs": []}, {"metadata": {"id": "04b7f9258df2489289889c0b23df0c74"}, "cell_type": "code", "source": "s_nb_trades = df_perf_bayes_best_only[\"equity_curve\"].apply(get_nb_trades)\ndf_perf_bayes_best_only[\"nb_trades\"] = s_nb_trades", "execution_count": 132, "outputs": [{"output_type": "stream", "text": "/tmp/1000120000/ipykernel_229/449762114.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_perf_bayes_best_only[\"nb_trades\"] = s_nb_trades\n", "name": "stderr"}]}, {"metadata": {"id": "04b7f9258df2489289889c0b23df0c74"}, "cell_type": "code", "source": "df_perf_bayes_best_only[\"expected_gain\"] = df_perf_bayes_best_only[\"nb_trades\"] * df_perf_bayes_best_only[\"expectancy\"]", "execution_count": 133, "outputs": [{"output_type": "stream", "text": "/tmp/1000120000/ipykernel_229/4293359956.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_perf_bayes_best_only[\"expected_gain\"] = df_perf_bayes_best_only[\"nb_trades\"] * df_perf_bayes_best_only[\"expectancy\"]\n", "name": "stderr"}]}, {"metadata": {"id": "6b0cca4d62664aba81de2f71f8dc0e0c"}, "cell_type": "code", "source": "df_perf_bayes_best_only = df_perf_bayes_best_only.sort_values(by=[\"expected_gain\",\"profit_factor\"], ascending=False)", "execution_count": 134, "outputs": []}, {"metadata": {"id": "0f72e01f4efc4ec8abfd83e32bc9ce3d"}, "cell_type": "code", "source": "df_perf_bayes_best_only.shape", "execution_count": 135, "outputs": [{"output_type": "execute_result", "execution_count": 135, "data": {"text/plain": "(517, 10)"}, "metadata": {}}]}, {"metadata": {"id": "4c70fed54e374043ba3bbc4317657019"}, "cell_type": "code", "source": "wslib = access_project_or_space()\nasset = wslib.save_data(\"Bayes_params_detailed_performance_Short_EG.csv\", data=str.encode(df_perf_bayes_best_only.to_csv(header=True, index=False)), overwrite=True)\nwslib.show(asset)", "execution_count": 136, "outputs": [{"output_type": "stream", "text": "{'name': 'Bayes_params_detailed_performance_Short_EG.csv',\n 'asset_type': 'data_asset',\n 'asset_id': 'ffb360ef-23f3-4200-a147-860b00631361',\n 'attachment_id': '274c716e-50d4-4b36-a641-8c2ccf513383',\n 'filepath': 'Bayes_params_detailed_performance_Short_EG.csv',\n 'data_size': None,\n 'mime': 'text/csv',\n 'summary': ['created or overwritten file',\n             'created data asset',\n             'created attachment']}\n", "name": "stdout"}]}, {"metadata": {"id": "dfb74525f55445699b8b33551abdde48"}, "cell_type": "markdown", "source": "# Tests fonctionnels"}, {"metadata": {"id": "d35921e006b64e498e03068e44875e35"}, "cell_type": "markdown", "source": "Transformation de la colonne params dict en colonnes"}, {"metadata": {"id": "bc1bce7192ac4519b9d406b2b6dec54e"}, "cell_type": "code", "source": "df_ucb.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "033e5d8755f54c068c7e222a9383a5ac"}, "cell_type": "code", "source": "type(df_ucb.iloc[0,1])", "execution_count": null, "outputs": []}, {"metadata": {"id": "06e9b2975ea243358d751bd54578521d"}, "cell_type": "code", "source": "df_unnested = df_ucb[\"params\"].apply(pd.Series)", "execution_count": null, "outputs": []}, {"metadata": {"id": "7785b3b0d32244a79035b80493ae8cc8"}, "cell_type": "code", "source": "df_ucb_unnested = df_ucb.join(df_unnested)\ndf_ucb_unnested.drop(columns=[\"params\"],inplace=True)\ndf_ucb_unnested.sort_values(by=[\"target\"], ascending=False, inplace=True)", "execution_count": 92, "outputs": []}, {"metadata": {"id": "1b0798c7dc7f4fa08d424b17f9b8501f"}, "cell_type": "code", "source": "df_ucb_unnested.head(1)", "execution_count": 71, "outputs": []}, {"metadata": {"id": "faa14017f0b940248162ba8a0905104b"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"colab": {"provenance": [], "collapsed_sections": ["J3HFB0dMU5gZ", "AHUieIds5BiL", "pTBQpw9u2Fzo", "XxWuUYApKXyq", "bJUTIaxyVz6B", "EbCqhjaFcBXo", "rYMgPLFf-WVU", "E3TtPoWKJZry"]}, "kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.7", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}